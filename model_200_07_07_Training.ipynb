{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05ca2c1e",
   "metadata": {},
   "source": [
    "### model_200_07_07 Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff9bb59",
   "metadata": {},
   "source": [
    "vector_size=200, window=7, min_count=7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac379dd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Chad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import string\n",
    "import nltk\n",
    "import contractions\n",
    "import unicodedata\n",
    "\n",
    "nltk.download('punkt')\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.probability import FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfe42264",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c2cf332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wikipedia-api in c:\\users\\chad\\anaconda3\\lib\\site-packages (0.6.0)\n",
      "Requirement already satisfied: requests in c:\\users\\chad\\anaconda3\\lib\\site-packages (from wikipedia-api) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\chad\\anaconda3\\lib\\site-packages (from requests->wikipedia-api) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\chad\\anaconda3\\lib\\site-packages (from requests->wikipedia-api) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\chad\\anaconda3\\lib\\site-packages (from requests->wikipedia-api) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\chad\\anaconda3\\lib\\site-packages (from requests->wikipedia-api) (2023.7.22)\n"
     ]
    }
   ],
   "source": [
    "!pip install wikipedia-api\n",
    "import wikipediaapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b980d11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "user = 'ChadBrowneProjectt/1.0 (david.freeborn@nulondon.ac.uk)'\n",
    "\n",
    "# 2) language to specify language mutation. It has to be one of supported languages e.g. 'en'\n",
    "\n",
    "wiki_wiki = wikipediaapi.Wikipedia(user, 'en')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbe2802",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4ad41ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_utf(text):\n",
    "    text = text.replace('\\u2018', \"'\").replace('\\u2019', \"'\").replace('\\u201C', \"`\").replace('\\u201D', \"`\").replace('\\u2013', '-').replace('\\u2014', '-')\n",
    "    text = unicodedata.normalize('NFKD', text)\n",
    "    text = text.encode('ascii', 'ignore')\n",
    "    return text.decode('ascii')\n",
    "\n",
    "def tokenise_sentences(text):\n",
    "    \n",
    "    text = convert_utf(text)\n",
    "    \n",
    "    \n",
    "    text_lowercase = text.lower()\n",
    "\n",
    "    \n",
    "    expanded_text = contractions.fix(text_lowercase)\n",
    "    \n",
    "    sentences = nltk.sent_tokenize(expanded_text)\n",
    "\n",
    "    data = []\n",
    "       \n",
    "    for sentence in sentences:\n",
    "        words = nltk.word_tokenize(sentence)\n",
    "        words = [ word for word in words if word.isalpha() and word not in stopwords.words('english')]\n",
    "\n",
    "        data.append(words)\n",
    "       \n",
    "        \n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeaef398",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "684f00a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(vector_size=200, window=7, min_count=7, sg=0)\n",
    "model.save(\"./model_200_07_07\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4096e96d",
   "metadata": {},
   "source": [
    "./model_200_07_07"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b3b6560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training on alice_in_wonderland.txt\n"
     ]
    }
   ],
   "source": [
    "file_names = [ \"alice_in_wonderland.txt\" ]\n",
    "\n",
    "\n",
    "for file_name in file_names:\n",
    "    with open(\"./as_files/\"+file_name, \"r\", encoding=\"utf8\") as file:    \n",
    "        content = file.read()\n",
    "\n",
    "    data = tokenise_sentences(content)\n",
    "\n",
    "    model.build_vocab(data, update=False)\n",
    "    model.train(data, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "    model.save('./model_200_07_07')\n",
    "    print(\"Finished training on \" + file_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a338705b",
   "metadata": {},
   "source": [
    "### Chat GPT2 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2d64abc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training on webtext.train.jsonl\n",
      "Finished training on webtext.valid.jsonl\n",
      "Finished training on webtext.test.jsonl\n",
      "Finished training on small-117M.train.jsonl\n",
      "Finished training on small-117M.valid.jsonl\n",
      "Finished training on small-117M.test.jsonl\n",
      "Finished training on small-117M-k40.train.jsonl\n",
      "Finished training on small-117M-k40.valid.jsonl\n",
      "Finished training on small-117M-k40.test.jsonl\n",
      "Finished training on medium-345M.train.jsonl\n",
      "Finished training on medium-345M.valid.jsonl\n",
      "Finished training on medium-345M.test.jsonl\n",
      "Finished training on medium-345M-k40.train.jsonl\n",
      "Finished training on medium-345M-k40.valid.jsonl\n",
      "Finished training on medium-345M-k40.test.jsonl\n",
      "Finished training on large-762M.train.jsonl\n",
      "Finished training on large-762M.valid.jsonl\n",
      "Finished training on large-762M.test.jsonl\n",
      "Finished training on large-762M-k40.train.jsonl\n",
      "Finished training on large-762M-k40.valid.jsonl\n",
      "Finished training on large-762M-k40.test.jsonl\n",
      "Finished training on xl-1542M.train.jsonl\n",
      "Finished training on xl-1542M.valid.jsonl\n",
      "Finished training on xl-1542M.test.jsonl\n",
      "Finished training on xl-1542M-k40.train.jsonl\n",
      "Finished training on xl-1542M-k40.valid.jsonl\n",
      "Finished training on xl-1542M-k40.test.jsonl\n"
     ]
    }
   ],
   "source": [
    "file_names = [ 'webtext.train.jsonl', 'webtext.valid.jsonl', 'webtext.test.jsonl', 'small-117M.train.jsonl', 'small-117M.valid.jsonl', 'small-117M.test.jsonl', 'small-117M-k40.train.jsonl', 'small-117M-k40.valid.jsonl', 'small-117M-k40.test.jsonl', 'medium-345M.train.jsonl', 'medium-345M.valid.jsonl', 'medium-345M.test.jsonl', 'medium-345M-k40.train.jsonl', 'medium-345M-k40.valid.jsonl', 'medium-345M-k40.test.jsonl', 'large-762M.train.jsonl', 'large-762M.valid.jsonl', 'large-762M.test.jsonl', 'large-762M-k40.train.jsonl', 'large-762M-k40.valid.jsonl', 'large-762M-k40.test.jsonl', 'xl-1542M.train.jsonl', 'xl-1542M.valid.jsonl', 'xl-1542M.test.jsonl', 'xl-1542M-k40.train.jsonl', 'xl-1542M-k40.valid.jsonl', 'xl-1542M-k40.test.jsonl' ]\n",
    "\n",
    "\n",
    "for file_name in file_names:\n",
    "    with open(\"./Chat_gpt_2/\"+file_name, \"r\", encoding=\"utf8\") as file: \n",
    "        for line in file:\n",
    "        \n",
    "            json_object = json.loads(line)\n",
    "            \n",
    "        \n",
    "        content = json_object.get(\"text\", \"\")\n",
    "        \n",
    "        data = tokenise_sentences(content)\n",
    "        \n",
    "        model.build_vocab(data, update=True)\n",
    "        model.train(data, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "        model.save(\"./model_200_07_07\")\n",
    "        print(\"Finished training on \" + file_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32e6807",
   "metadata": {},
   "source": [
    "### Encyclopedia Britannica dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc35a7f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training on encyclopedia_britannica_01.txt\n",
      "Finished training on encyclopedia_britannica_02.txt\n",
      "Finished training on Encyclopedia_britannica_03.txt\n",
      "Finished training on Encyclopedia_britannica_04.txt\n",
      "Finished training on Encyclopedia_britannica_05.txt\n",
      "Finished training on Encyclopedia_britannica_06.txt\n",
      "Finished training on Encyclopedia_britannica_07.txt\n",
      "Finished training on Encyclopedia_britannica_08.txt\n",
      "Finished training on Encyclopedia_britannica_09.txt\n",
      "Finished training on Encyclopedia_britannica_10.txt\n",
      "Finished training on Encyclopedia_britannica_11.txt\n",
      "Finished training on Encyclopedia_britannica_12.txt\n",
      "Finished training on Encyclopedia_britannica_13.txt\n",
      "Finished training on Encyclopedia_britannica_14.txt\n",
      "Finished training on Encyclopedia_britannica_15.txt\n",
      "Finished training on Encyclopedia_britannica_16.txt\n",
      "Finished training on Encyclopedia_britannica_17.txt\n",
      "Finished training on Encyclopedia_britannica_18.txt\n",
      "Finished training on Encyclopedia_britannica_19.txt\n",
      "Finished training on Encyclopedia_britannica_20.txt\n",
      "Finished training on Encyclopedia_britannica_21.txt\n",
      "Finished training on Encyclopedia_britannica_22.txt\n",
      "Finished training on Encyclopedia_britannica_23.txt\n",
      "Finished training on Encyclopedia_britannica_24.txt\n",
      "Finished training on Encyclopedia_britannica_25.txt\n",
      "Finished training on Encyclopedia_britannica_26.txt\n",
      "Finished training on Encyclopedia_britannica_27.txt\n",
      "Finished training on Encyclopedia_britannica_28.txt\n",
      "Finished training on Encyclopedia_britannica_29.txt\n"
     ]
    }
   ],
   "source": [
    "file_names = [ \"encyclopedia_britannica_01.txt\",\"encyclopedia_britannica_02.txt\", \"Encyclopedia_britannica_03.txt\", \"Encyclopedia_britannica_04.txt\", \"Encyclopedia_britannica_05.txt\", \"Encyclopedia_britannica_06.txt\", \"Encyclopedia_britannica_07.txt\", \"Encyclopedia_britannica_08.txt\", \"Encyclopedia_britannica_09.txt\", \"Encyclopedia_britannica_10.txt\", \"Encyclopedia_britannica_11.txt\", \"Encyclopedia_britannica_12.txt\", \"Encyclopedia_britannica_13.txt\", \"Encyclopedia_britannica_14.txt\", \"Encyclopedia_britannica_15.txt\", \"Encyclopedia_britannica_16.txt\", \"Encyclopedia_britannica_17.txt\", \"Encyclopedia_britannica_18.txt\", \"Encyclopedia_britannica_19.txt\", \"Encyclopedia_britannica_20.txt\", \"Encyclopedia_britannica_21.txt\", \"Encyclopedia_britannica_22.txt\", \"Encyclopedia_britannica_23.txt\", \"Encyclopedia_britannica_24.txt\",\"Encyclopedia_britannica_25.txt\", \"Encyclopedia_britannica_26.txt\", \"Encyclopedia_britannica_27.txt\", \"Encyclopedia_britannica_28.txt\", \"Encyclopedia_britannica_29.txt\", ]\n",
    "\n",
    "\n",
    "for file_name in file_names:\n",
    "    with open(\"./as_files/\"+file_name, \"r\", encoding=\"utf8\") as file:    \n",
    "        content = file.read()\n",
    "\n",
    "    data = tokenise_sentences(content)\n",
    "\n",
    "    model.build_vocab(data, update=True)\n",
    "    model.train(data, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "    model.save(\"./model_200_07_07\")\n",
    "    print(\"Finished training on \" + file_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9217fc84",
   "metadata": {},
   "source": [
    "### Books dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5c41d3aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training on beyond_good_and_evil.txt\n",
      "Finished training on crime_and_punishment.txt\n",
      "Finished training on frankenstein.txt\n",
      "Finished training on great_expectations.txt\n",
      "Finished training on little_women.txt\n",
      "Finished training on moby_dick.txt\n",
      "Finished training on pride_and_prejudice.txt\n",
      "Finished training on six_metaphysical_meditations.txt\n",
      "Finished training on the_complete_works_of_william_shakespeare.txt\n",
      "Finished training on the_great_gatsby.txt\n",
      "Finished training on the_picture_of_dorian_gray.txt\n",
      "Finished training on the_republic.txt\n",
      "Finished training on war_and_peace.txt\n",
      "Finished training on my_life.txt\n",
      "Finished training on the_scarlet_letter.txt\n",
      "Finished training on dracula.txt\n",
      "Finished training on the_prince.txt\n",
      "Finished training on the_souls_of_black_folk.txt\n",
      "Finished training on adventures_of_huckleberry.txt\n",
      "Finished training on the_odyssey.txt\n",
      "Finished training on peter_pan.txt\n",
      "Finished training on narrative_of_the_life_of_frederick_douglass.txt\n",
      "Finished training on emma.txt\n",
      "Finished training on oliver_twist.txt\n",
      "Finished training on the_time_machine.txt\n",
      "Finished training on through_the_looking_glass.txt\n",
      "Finished training on the_problems_of_philosophy.txt\n",
      "Finished training on the_secret_garden.txt\n",
      "Finished training on the_communist_manifesto.txt\n",
      "Finished training on the_importance_of_being_earnest.txt\n"
     ]
    }
   ],
   "source": [
    "file_names = [ \"beyond_good_and_evil.txt\", \"crime_and_punishment.txt\", \"frankenstein.txt\", \"great_expectations.txt\", \"little_women.txt\", \"moby_dick.txt\", \"pride_and_prejudice.txt\", \"six_metaphysical_meditations.txt\", \"the_complete_works_of_william_shakespeare.txt\", \"the_great_gatsby.txt\", \"the_picture_of_dorian_gray.txt\", \"the_republic.txt\", \"war_and_peace.txt\", \"my_life.txt\", \"the_scarlet_letter.txt\", \"dracula.txt\", \"the_prince.txt\", \"the_souls_of_black_folk.txt\", \"adventures_of_huckleberry.txt\", \"the_odyssey.txt\", \"peter_pan.txt\", \"narrative_of_the_life_of_frederick_douglass.txt\", \"emma.txt\", \"oliver_twist.txt\", \"the_time_machine.txt\", \"through_the_looking_glass.txt\", \"the_problems_of_philosophy.txt\", \"the_secret_garden.txt\", \"the_communist_manifesto.txt\", \"the_importance_of_being_earnest.txt\", ]\n",
    "\n",
    "\n",
    "for file_name in file_names:\n",
    "    with open(\"./as_files/\"+file_name, \"r\", encoding=\"utf8\") as file:    \n",
    "        content = file.read()\n",
    "\n",
    "    data = tokenise_sentences(content)\n",
    "\n",
    "    model.build_vocab(data, update=True)\n",
    "    model.train(data, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "    model.save('./model_200_07_07')\n",
    "    print(\"Finished training on \" + file_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b332cc9",
   "metadata": {},
   "source": [
    "### Wikipedia Dumps dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "899a7602",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wikipages_content(page_titles):\n",
    "    combined_text = \"\"\n",
    "\n",
    "    for page_title in page_titles:\n",
    "        page = wiki_wiki.page(page_title)\n",
    "        if page.exists():\n",
    "            page_content = page.text\n",
    "            combined_text += page_content \n",
    "\n",
    "    return combined_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "db356d50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training on all Wikipedia pages.\n"
     ]
    }
   ],
   "source": [
    "page_titles = ['Elephant',\n",
    "    'Artificial neural network',\n",
    "    'Ancient Egypt',\n",
    "    'Renewable energy',\n",
    "    'Vincent van Gogh',\n",
    "    'Quantum physics',\n",
    "    'Human spaceflight',\n",
    "    'popular music',\n",
    "    'Civil liberties',\n",
    "    'Oceanography',\n",
    "    'Wind power',\n",
    "    'Cognitive science',\n",
    "    'Ancient Greece',\n",
    "    'Computer vision',\n",
    "    'evolution',\n",
    "    'Surrealist automatism',\n",
    "    'Rainforest ',\n",
    "    'ecology ', \n",
    "    'Robotics',\n",
    "    'kid cudi',\n",
    "    'Microeconomics',\n",
    "    'Genomics',\n",
    "    ' thermometer', ' boat', 'House',\n",
    "    'post malone',\n",
    "    'Hydropower',\n",
    "    'Black holes',\n",
    "    'Baroque architecture', 'Barbecue', \n",
    "    'Solar energy', 'Solar  deity', \n",
    "    'Planetary science',\n",
    "    'Behavioral psychology',\n",
    "    'World War I', 'war', 'north', 'sout', 'east', 'west',\n",
    "    'Deep learning', 'Chadwick Boseman', 'transgender', ''\n",
    "    'Cybersecurity', 'Transhumanism', 'school', \n",
    "    'Pop art', 'Art movement', 'Sustainable architecture'\n",
    "    'Cultural diversity', 'light', 'Light-emitting diode', 'LED display',\n",
    "    'Green architecture', 'Darkness', 'Jellyfish', \n",
    "    'Cellular biology', 'The Simpsons', \n",
    "    'International law',\n",
    "    'Stem cell research',\n",
    "    'Supernovae',\n",
    "    'Neurophilosophy',\n",
    "    'African literature',\n",
    "    'Economic inequality', 'Poverty', 'rich', 'king', 'queen', 'father', 'mother','child', 'daughter'\n",
    "    'Quantum computing',\n",
    "    'Medieval history',\n",
    "    'Environmental conservation',\n",
    "    'Bioinformatics',\n",
    "    'Particle accelerators',\n",
    "    'Organic farming',\n",
    "    'Opera',\n",
    "    'Epidemiology',\n",
    "    'Contemporary art', 'Happiness', 'sad'\n",
    "    'Philosophy of language',\n",
    "    'Computer networks',\n",
    "    'Astronomical observation',\n",
    "    'Psycholinguistics',\n",
    "    'African music',\n",
    "    'Feminist literature',\n",
    "    'Game design',\n",
    "    'Genetic diversity',\n",
    "    'Industrial design', ]\n",
    "\n",
    "\n",
    "text = get_wikipages_content(page_titles)\n",
    "\n",
    "t\n",
    "data = tokenise_sentences(text)\n",
    "\n",
    "\n",
    "model.build_vocab(data, update=True)\n",
    "model.train(data, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "\n",
    "\n",
    "model.save(\"./model_200_07_07\")\n",
    "\n",
    "\n",
    "print(\"Finished training on all Wikipedia pages.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b420aa",
   "metadata": {},
   "source": [
    "### Wikipedia Dumps opposites dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "38a06b98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training on all Wikipedia pages.\n"
     ]
    }
   ],
   "source": [
    "page_titles =  [\n",
    "    'light', 'dark',\n",
    "    'big', 'small',\n",
    "    'fast', 'slow',\n",
    "    'high', 'low',\n",
    "    'hot', 'cold',\n",
    "    'hard', 'soft',\n",
    "    'old', 'young',\n",
    "    'rich', 'poor',\n",
    "    'thick', 'thin',\n",
    "    'deep', 'shallow',\n",
    "    'wide', 'narrow',\n",
    "    'empty', 'full',\n",
    "    'strong', 'weak',\n",
    "    'loud', 'quiet',\n",
    "    'happy', 'sad',\n",
    "    'thirsty', 'full',\n",
    "    'north', 'south',\n",
    "    'east', 'west',\n",
    "    'up', 'down',\n",
    "    'first', 'last',\n",
    "    'day', 'night',\n",
    "    'summer', 'winter',\n",
    "    'bright', 'dull',\n",
    "    'clean', 'dirty',\n",
    "    'easy', 'difficult',\n",
    "    'fresh', 'stale',\n",
    "    'forward', 'backward',\n",
    "    'modern', 'ancient',\n",
    "    'positive', 'negative',\n",
    "    'new', 'old',\n",
    "    'empty', 'full',\n",
    "    'simple', 'complex',\n",
    "    'beginning', 'end',\n",
    "    'smooth', 'rough',\n",
    "    'near', 'far',\n",
    "    'happy', 'sad',\n",
    "    'increase', 'decrease',\n",
    "    'strong', 'weak',\n",
    "    'wide', 'narrow',\n",
    "    'thick', 'thin',\n",
    "    'float', 'sink',\n",
    "    'empty', 'occupied',\n",
    "    'arrival', 'departure',\n",
    "    'build', 'destroy',\n",
    "    'expand', 'contract',\n",
    "    'bright', 'dim',\n",
    "    'forward', 'backward',\n",
    "    'full', 'empty',\n",
    "    'gain', 'loss',\n",
    "    'hard', 'soft',\n",
    "    'harmony', 'discord',\n",
    "    'heaven', 'hell',\n",
    "    'innocence', 'guilt',\n",
    "    'invite', 'repel',\n",
    "    'join', 'separate',\n",
    "    'kind', 'cruel',\n",
    "    'laugh', 'cry',\n",
    "    'liberty', 'oppression',\n",
    "    'love', 'hate',\n",
    "    'mend', 'break',\n",
    "    'merge', 'split',\n",
    "    'natural', 'artificial',\n",
    "    'obscure', 'clear',\n",
    "    'order', 'chaos',\n",
    "    'patience', 'impatience',\n",
    "    'pleasure', 'pain',\n",
    "    'polite', 'rude',\n",
    "    'prosperity', 'adversity',\n",
    "    'push', 'pull',\n",
    "    'rare', 'common',\n",
    "    'repair', 'damage',\n",
    "    'rise', 'fall',\n",
    "    'save', 'spend',\n",
    "    'silent', 'noisy',\n",
    "    'simple', 'complicated',\n",
    "    'soft', 'hard',\n",
    "    'succeed', 'fail',\n",
    "    'teach', 'learn',\n",
    "    'together', 'apart',\n",
    "    'victory', 'defeat',\n",
    "    'visible', 'invisible',\n",
    "    'win', 'lose',\n",
    "    'young', 'old',\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "text = get_wikipages_content(page_titles)\n",
    "\n",
    "\n",
    "data = tokenise_sentences(text)\n",
    "\n",
    "\n",
    "model.build_vocab(data, update=True)\n",
    "model.train(data, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "\n",
    "\n",
    "model.save(\"./model_200_07_07\")\n",
    "\n",
    "\n",
    "print(\"Finished training on all Wikipedia pages.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81b605f",
   "metadata": {},
   "source": [
    "changing vecotr size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca46e26d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "71dfd669",
   "metadata": {},
   "source": [
    "### test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "17f98805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('moscheles', -0.2750222980976105), ('catechisms', -0.2677824795246124), ('nicomachean', -0.2584400773048401), ('evangelical', -0.25600558519363403), ('universalist', -0.25565820932388306)]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "all_sims = model.wv.most_similar('sea', topn=sys.maxsize)\n",
    "last_5 = list(reversed(all_sims[-5:]))\n",
    "print(last_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2bf69881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('calibration', 0.2759140729904175), ('legations', 0.2587907910346985), ('tween', 0.25851303339004517), ('tinuous', 0.2565498352050781), ('guese', 0.23949001729488373)]\n"
     ]
    }
   ],
   "source": [
    "print(  model.wv.most_similar(negative=[\"man\"], topn=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "69e38c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = model.wv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bceca8f",
   "metadata": {},
   "source": [
    "### Simular "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44adef9",
   "metadata": {},
   "source": [
    "Chat GPT2 Dataset Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488aaf39",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(  model.wv.most_similar('man', topn=5)  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8cfcfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(  model.wv.most_similar('north', topn=5)  )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f2d173",
   "metadata": {},
   "source": [
    "Encyclopedia Britannica "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607facd3",
   "metadata": {},
   "source": [
    "Vectors 200 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4a56681f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "man:  [ 0.82013696 -1.0821011   0.9925261   1.1949201   0.6460428  -0.49452963\n",
      "  1.2682606   1.5284575  -1.2033851  -0.98638177 -0.5289169  -0.17877641\n",
      " -1.6252681   0.6673063   1.0768359   0.5729542   0.64762896  0.5356137\n",
      " -0.06576855 -0.34480205 -0.23337479  0.22363634 -1.6157396  -1.5892632\n",
      " -1.8646302  -0.59702015 -0.96094954  2.8437634   1.5588413   0.62771076\n",
      "  0.85997915  0.3683359   0.59237593 -2.5237157   1.6005946   0.5691209\n",
      "  1.0359842   0.71972823 -0.35975116  1.109989    0.14258772 -0.92315876\n",
      "  0.17137998 -1.5770068   0.76803     1.1510135  -1.2654456  -0.92741066\n",
      "  0.2567056   0.9886807   0.93157816  0.43126628 -0.88573855  0.05342961\n",
      " -0.02345959  0.5554634   0.33121738 -0.9309308  -1.2607929  -0.16353422\n",
      "  0.87766516 -2.466607    0.01829967 -1.4417275   1.5932248   1.3650799\n",
      "  1.9540309  -0.8819071  -1.7866335  -1.4165764  -0.30081865 -0.13608482\n",
      "  0.6976375   0.2633169  -0.2631721   1.3872229   1.6076182   0.26229605\n",
      " -0.44009688 -1.309223   -0.30516452 -2.2341826  -0.78879863 -0.83761185\n",
      " -0.60551953  1.5841694   1.1134061  -2.3422146  -2.1242213   0.29681414\n",
      "  0.16904977  1.0149556  -0.5712817  -0.18817998 -0.4327587  -0.5119095\n",
      " -0.8760313   0.7710435  -0.9858362  -0.56257206 -1.66159     0.18389884\n",
      " -0.03890301 -0.8786857  -0.77923495 -0.10930452  1.5098761   0.41585934\n",
      "  0.6329419   2.2089262  -0.8018948   0.7857834  -1.65728    -0.29893243\n",
      " -0.35186675 -0.4808016  -0.63887036  0.521191   -0.43476188 -0.85019666\n",
      " -0.1485482   0.7888719   0.9318004  -0.49721083 -0.6401153   0.8330293\n",
      " -1.2829221  -0.10654869  0.09606088 -1.5250105   0.5793117  -0.98373824\n",
      " -1.0056617  -0.22952068 -0.1936485  -1.1391464  -0.3596001  -0.4720408\n",
      "  0.9918666   0.7465705   0.7813522   2.2255502   1.1953804   0.7347467\n",
      "  0.9276889   0.4012379  -0.26138046  0.14629635  0.35007077 -1.8494914\n",
      " -0.9055942  -0.22690856 -0.01667866 -0.10831197 -0.37237802  0.3047134\n",
      "  1.2563896   1.219032    1.0753975   0.48641372  0.50607896  1.8911418\n",
      " -0.51653177  0.28282464 -0.06096584 -0.0677098  -0.68551743 -0.94662714\n",
      " -0.4303588   0.49988908  0.4336168   1.2835982   0.48161983 -0.5432825\n",
      "  1.1640195   1.0876155  -0.8665151   0.6876513  -1.6835567  -1.3498831\n",
      "  0.7295731   0.5851322   0.30720428 -0.7723756   0.26821044 -0.7145905\n",
      " -0.4826951   0.18016973  0.91765755 -0.1467048  -0.3591338  -0.4112062\n",
      " -0.32242003  1.6541908   0.9699508  -0.6328416  -0.59276766  0.9651971\n",
      " -0.01925338  0.4526858 ]\n",
      "[('woman', 0.5394182801246643), ('men', 0.5261871218681335), ('virtuous', 0.5167989134788513), ('creature', 0.5160272717475891), ('humble', 0.5002942085266113)]\n"
     ]
    }
   ],
   "source": [
    "print( \"man:  \" +str(word_vectors['man'])  )\n",
    "\n",
    "print(  model.wv.most_similar('man', topn=5)  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d9ace9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print( \"North:  \" +srt(word_vector['north'])  )\n",
    "print(  model.wv.most_similar('north', topn=5)  )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a96815",
   "metadata": {},
   "source": [
    "Vector 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e80e819",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aff8370",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2cfd46e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "febcb7bd",
   "metadata": {},
   "source": [
    "Book Dataset Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "21b7ed5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "man:  [ 1.1477364e+00  8.7682194e-01 -2.1483445e-01  1.1715766e+00\n",
      "  5.2508277e-01 -3.7006459e-01  5.7774615e-01  1.1514508e+00\n",
      " -3.9303580e-01 -8.8179618e-01 -7.9761118e-01  4.2790213e-01\n",
      " -4.8926571e-01 -7.9403412e-01 -1.7862171e-02  5.4748160e-01\n",
      "  7.8771263e-01  6.8002135e-01  5.2264637e-01  8.7217718e-02\n",
      "  2.0616020e-01 -4.5871813e-02 -1.7631894e+00 -1.2155520e+00\n",
      " -1.5302579e+00  2.4157573e-01 -7.6659536e-01  1.6934192e+00\n",
      " -2.5147686e-02  6.0627282e-01 -4.4513351e-01 -6.5677673e-01\n",
      "  4.5915857e-01 -5.2425456e-01  1.0247065e+00 -6.6604391e-02\n",
      "  1.1542416e+00  1.1311901e+00  5.2019864e-02  1.2353624e+00\n",
      "  5.3953713e-01 -7.5909102e-01  2.7963224e-01 -7.8983164e-01\n",
      "  1.2182716e-01  5.8874977e-01 -5.5204755e-01 -5.9523344e-01\n",
      "  1.7824022e+00  6.9650489e-01  1.6032186e+00 -5.7513815e-01\n",
      " -5.2011156e-01 -6.0053360e-01 -5.5214840e-01  1.6660184e-01\n",
      "  5.0676775e-01  5.3074300e-01 -1.8980018e+00  6.9613442e-02\n",
      " -3.0151984e-01 -1.0952520e+00  1.1212778e-02 -5.3913437e-02\n",
      "  1.1795796e+00  9.2365700e-01  1.9398412e-01  5.7349372e-01\n",
      " -1.3264817e+00 -1.3332853e+00  8.1924750e-03 -1.0975199e+00\n",
      " -3.7804028e-01  8.3480257e-01 -1.7460279e-01 -1.0120667e-01\n",
      "  4.1001585e-01  1.5912886e+00  1.2397271e-02 -4.4924641e-01\n",
      "  2.8195474e-01 -1.1786087e+00 -2.8799313e-01 -1.7037378e-01\n",
      "  2.9304957e-01  1.9688793e+00  1.0815580e+00 -5.6808203e-01\n",
      " -1.1206031e+00  9.9049872e-01  5.0051922e-01 -2.1044074e-01\n",
      " -5.0488293e-01 -1.8692254e-01 -6.9469206e-02  5.5922067e-01\n",
      "  2.5063727e-02 -9.8161800e-03 -2.8875613e-01 -6.4631468e-01\n",
      " -1.1215335e+00  2.4576785e-01 -9.1258961e-01 -4.6786067e-01\n",
      "  2.2036964e-01  1.6486928e-03  9.8780406e-01  1.3395188e+00\n",
      "  7.5689137e-01  7.7445924e-01 -2.6517934e-01  9.6315312e-01\n",
      " -1.2454007e+00 -9.4686516e-02  6.9596422e-01  9.8754162e-01\n",
      " -1.0468411e+00 -1.0147352e-02  9.9572621e-04  2.3563795e-02\n",
      "  2.6178199e-01 -4.1669264e-01  7.8681880e-01 -6.1991565e-02\n",
      "  1.5000412e-01 -2.1256331e-01 -6.6218966e-01 -1.7454620e-01\n",
      " -7.1553446e-02  9.0920255e-02 -1.6955665e-01 -7.4387741e-01\n",
      "  5.0751758e-01 -3.8119513e-01  3.2107748e-02 -3.0985403e-01\n",
      "  2.3587036e-01 -6.6593510e-01  5.5615300e-01  5.1060843e-01\n",
      "  1.1069709e+00  1.3789603e+00  1.8141304e+00  1.0737965e+00\n",
      "  8.1003594e-01  1.3146713e+00 -1.3110785e-01 -5.9523112e-01\n",
      "  3.1554627e-01 -2.6538986e-01 -9.1575623e-02  2.2832254e-01\n",
      " -1.3096082e-01 -4.8083413e-01 -5.5699182e-01 -2.1136396e-01\n",
      "  6.3614845e-01  1.5897955e+00  8.0203468e-01  7.3635161e-02\n",
      " -3.3493924e-01  1.2251176e+00 -1.2692016e-01  1.1954401e+00\n",
      " -5.7781726e-01 -2.1361451e-01 -9.8364896e-01 -6.9199014e-01\n",
      "  3.1491289e-01  3.8182494e-01  9.9050164e-01 -5.7261392e-02\n",
      "  6.4916688e-01 -7.4160993e-01  7.4824983e-01  9.3925923e-01\n",
      " -5.3491247e-01  6.7726654e-01 -3.3441800e-01 -7.5326759e-01\n",
      "  1.5012540e-01  1.1046722e+00  4.8699012e-01  1.8347749e-01\n",
      " -3.9826673e-01  5.2141494e-01 -9.6229208e-01 -1.4775109e-01\n",
      "  6.0584661e-02 -1.3160443e-01 -1.0885889e-01  9.4561048e-02\n",
      " -7.3911798e-01  9.8144007e-01  1.5357131e+00 -6.2595147e-01\n",
      " -8.1375551e-01 -2.2333495e-01  1.7001928e-03 -5.7553917e-01]\n",
      "[('woman', 0.5792030096054077), ('creature', 0.4640474319458008), ('gentleman', 0.44806328415870667), ('boy', 0.421964168548584), ('lad', 0.41272592544555664)]\n"
     ]
    }
   ],
   "source": [
    "print( \"man:  \" +str(word_vectors['man'])  )\n",
    "\n",
    "print(  model.wv.most_similar('man', topn=5)  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85f533d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a62ec342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('guaira', 0.2568216323852539), ('chronological', 0.25575152039527893), ('calendars', 0.24891524016857147), ('portant', 0.24712257087230682), ('enlargements', 0.24616925418376923)]\n"
     ]
    }
   ],
   "source": [
    "print(  model.wv.most_similar(negative=[\"man\"], topn=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b510935",
   "metadata": {},
   "source": [
    "Wikipedia Dumps "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f693be65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "man:  [ 0.94734347  0.8668987   0.1775386   1.9829733   0.67119145  0.08957545\n",
      "  0.37597457  1.0907488  -0.02744303 -0.86449796 -1.4561753   0.15983403\n",
      " -0.14926499 -0.84886396  0.42431065  0.95947284  0.7633599   0.3829058\n",
      "  0.36770338  0.37337017  0.3917997  -0.06201517 -2.073991   -1.8447152\n",
      " -1.7930511   0.03872269 -0.5503579   1.8387884   0.5582387   0.62241566\n",
      " -0.3866386  -0.42011625  0.9078716  -0.46142587  1.2811668  -0.34915724\n",
      "  0.9152318   1.7303083  -0.2908047   1.4234322   0.27981597 -0.7458262\n",
      "  0.4765173  -1.0318983   0.12546115  0.85394883 -0.3447158  -0.63114107\n",
      "  1.6482445   0.7908918   1.7361327  -0.26490268 -0.51721644 -0.5374015\n",
      " -0.5530295   0.6953224   0.60312855  0.7276508  -1.9885281  -0.05688499\n",
      " -0.15432924 -1.5515265   0.19009861  0.19243366  0.69194055  0.73632807\n",
      "  0.81855315  0.5522702  -1.0688701  -1.2290937   0.15137403 -1.1504885\n",
      " -0.41778782  0.85450715 -0.261917   -0.01369009  0.2435376   1.7772933\n",
      "  0.05620319 -0.71010184  0.37843695 -1.3405406  -0.12254327  0.24634795\n",
      "  0.383041    1.7007902   1.1796215  -0.7300345  -0.6917714   0.5784556\n",
      "  1.1070127   0.15190591 -0.24242833 -0.11812504  0.3595443   0.10941573\n",
      " -0.5810606  -0.34175262 -0.32474843 -0.6275562  -1.4694419   0.24397904\n",
      " -1.5867934  -0.62633693  0.28796074  0.9178567   1.2410766   0.95539266\n",
      "  0.13058256  1.010927   -0.09942579  1.1266274  -1.6507814  -0.25463903\n",
      "  0.12015993  0.93187046 -1.239754    0.3533363   0.42266637  0.29073837\n",
      "  0.40464088  0.095586    0.18055542  0.18795295  0.61785305 -0.23582846\n",
      " -0.60750496 -0.32157508 -0.60089266 -0.0032925   0.36083016 -0.75076234\n",
      "  0.06792346  0.07686462 -0.2203081  -0.11470265 -0.18510023 -0.6678981\n",
      "  0.4161637   0.799064    0.5707811   1.5876839   1.5695868   1.7127037\n",
      "  1.1487223   1.5578403   0.27479953 -0.98123175  0.34569922  0.00953661\n",
      " -0.5154513   0.6272852   0.08345521 -0.6700457  -0.25366858  0.0150286\n",
      "  0.14158456  1.5879122   0.21897869  0.35709858 -0.455766    1.3691791\n",
      " -0.44410083  1.6316233  -0.26638994 -0.9209647  -0.98122543 -0.47201398\n",
      "  0.4013286  -0.04732375  1.4577557   0.23963206  0.6468369  -0.8229656\n",
      "  0.7282243   0.26031983 -0.38301995  0.54219127 -0.4864435  -0.6255326\n",
      " -0.02677018  1.1259038   0.8473957   0.8249017  -0.16124208  0.37819126\n",
      " -1.2545401   0.19073091 -0.11164495 -0.48258454 -0.25976774 -0.25554442\n",
      " -0.18233083  1.3750219   1.3592663  -0.5227989  -0.85096014  0.36917648\n",
      "  0.24890527 -0.46271738]\n",
      "[('woman', 0.5916791558265686), ('gentleman', 0.4517533481121063), ('boy', 0.43355557322502136), ('girl', 0.4104384183883667), ('lad', 0.4089280366897583)]\n"
     ]
    }
   ],
   "source": [
    "print( \"man:  \" +str(word_vectors['man'])  )\n",
    "\n",
    "print(  model.wv.most_similar('man', topn=5)  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "89875337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('enlargements', 0.2910909652709961), ('files', 0.2495306432247162), ('herculaneum', 0.24393679201602936), ('valletta', 0.24386689066886902), ('headings', 0.24203142523765564)]\n"
     ]
    }
   ],
   "source": [
    "print(  model.wv.most_similar(negative=[\"man\"], topn=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c73563a",
   "metadata": {},
   "source": [
    "Wikipedia Dumps  opposite titles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "049b596a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "man:  [ 0.85239995  0.8366422  -0.19352494  1.8518813   0.5989597   0.02387953\n",
      "  0.35271502  1.1938938   0.09731169 -0.8330351  -1.7163368   0.31075838\n",
      "  0.10637975 -1.137315    0.49296546  0.7826467   0.86865056  0.354108\n",
      "  0.46003577  0.55846775  0.21375044 -0.09258237 -2.036829   -1.878084\n",
      " -1.89961     0.03357806 -0.59514064  1.9511739   0.6295699   0.879147\n",
      " -0.60609716 -0.35829976  1.2168218  -0.6614034   1.1642306  -0.6244121\n",
      "  1.0938152   1.9339983  -0.4509004   1.5699358   0.4509434  -0.7712648\n",
      "  0.4145471  -1.3377258   0.3597039   1.0831646  -0.42732972 -0.90021807\n",
      "  1.7546612   0.6569792   1.9327209  -0.04867852 -0.4412063  -0.46030378\n",
      " -0.3829068   0.7069459   0.41823837  0.7624305  -1.8463669  -0.06139909\n",
      "  0.05002754 -1.5152471   0.39832026  0.02459382  0.7119997   0.8307434\n",
      "  0.6591287   0.2820274  -1.2671305  -1.1768267   0.08439332 -1.0736015\n",
      " -0.36922175  0.74870163 -0.28008947  0.26902154  0.49224645  1.7564999\n",
      "  0.24745785 -0.8315369   0.40481156 -1.1211618   0.25063193  0.24284618\n",
      "  0.32196644  1.8798095   1.4264882  -0.60102654 -0.77048177  0.7260306\n",
      "  1.219242   -0.06011673 -0.477545   -0.07453755  0.44349298 -0.13398069\n",
      " -0.82493186 -0.08990106 -0.32023814 -0.6775926  -1.3893638   0.09368483\n",
      " -1.4288434  -0.61985046  0.30415514  1.0424218   1.3171885   1.1017227\n",
      "  0.09107263  1.2843276  -0.32313722  1.1723989  -2.0650327  -0.29452744\n",
      " -0.04372272  0.8608317  -1.2361746   0.20205468  0.682427    0.17663941\n",
      "  0.47794968  0.22419451  0.25984302  0.22801739  0.670386   -0.1586378\n",
      " -0.6591668  -0.18410556 -0.7140657  -0.10443737  0.36979112 -0.5096325\n",
      "  0.11213256  0.08831363 -0.1285215  -0.14957704  0.06597035 -0.74790823\n",
      "  0.37982148  0.87234676  0.71806043  1.8833258   1.8657066   1.7839472\n",
      "  0.9393609   1.3409487   0.47722605 -0.8723249   0.48265934 -0.04326372\n",
      " -0.46013376  0.46380267  0.30200073 -0.10260814 -0.62002414  0.26029938\n",
      "  0.4637965   1.5794258   0.17206776  0.50655925 -0.41650695  1.1299924\n",
      " -0.53883135  1.474273   -0.02558385 -0.8844944  -0.7103182  -0.5489468\n",
      "  0.24453682  0.21155119  1.315012    0.21140856  0.91704726 -1.1842422\n",
      "  0.6506565   0.2913963  -0.575153    0.06899887 -0.7683881  -0.79246104\n",
      "  0.04402242  1.3136957   0.74973613  1.0738854   0.15234205  0.308143\n",
      " -1.5348527   0.09859003 -0.22450288 -0.3415979  -0.06941213 -0.10269134\n",
      " -0.20066443  1.4513958   1.3290236  -0.52627677 -0.759244    0.522087\n",
      "  0.40432158 -0.47022018]\n",
      "[('woman', 0.5915812849998474), ('gentleman', 0.45488935708999634), ('boy', 0.44908660650253296), ('lad', 0.42764711380004883), ('girl', 0.4126492738723755)]\n"
     ]
    }
   ],
   "source": [
    "print( \"man:  \" +str(word_vectors['man'])  )\n",
    "\n",
    "print(  model.wv.most_similar('man', topn=5)  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8feb7c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('enlargements', 0.30172044038772583)]\n"
     ]
    }
   ],
   "source": [
    "print(  model.wv.most_similar(negative=[\"man\"], topn=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b439de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498fb8d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4cba55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b65e5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5191f96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a3dbbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad368afb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18595abf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
